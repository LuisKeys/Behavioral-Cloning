{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV...\n",
      "Amount of data items:61524\n",
      "Amount of train items:49218\n",
      "Amount of validation items:12306\n",
      "Reading output values and images...\n",
      "Training NN...\n",
      "Epoch 1/6\n",
      "  3/480 [..............................] - ETA: 2603s - loss: 0.5057"
     ]
    }
   ],
   "source": [
    "# modules import section\n",
    "import csv\n",
    "import cv2\n",
    "import sklearn\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Conv2D, Cropping2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# import data\n",
    "# data buffers\n",
    "input_images = []\n",
    "samples = []\n",
    "output_values = []\n",
    "base_path = './data/'\n",
    "data_file_csv = open(base_path + 'driving_log.csv')\n",
    "data_reader = csv.reader(data_file_csv)\n",
    "first_line = True\n",
    "factor_for_pre_process = 6 #Get 3 images per line and flipped version = 6 images per CSV line\n",
    "# read csv file\n",
    "print('Reading CSV...')\n",
    "for line in data_reader:\n",
    "    if first_line:\n",
    "        first_line = False\n",
    "    else:\n",
    "        samples.append(line)\n",
    "                     \n",
    "print('Amount of data items:' + str(len(samples) * factor_for_pre_process))\n",
    "\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "print('Amount of train items:' + str(len(train_samples) * factor_for_pre_process))\n",
    "print('Amount of validation items:' + str(len(validation_samples) * factor_for_pre_process))\n",
    "\n",
    "# read images and output values\n",
    "print('Reading output values and images...')\n",
    "\n",
    "# Correction for perspective of different camaras frmo different angles\n",
    "steering_correction = 0.2\n",
    "cropped_height = 66\n",
    "cropped_width = 320\n",
    "batch_size=2048\n",
    "# Format image to [66, 320, 1]\n",
    "def format_image(X):\n",
    "    X_proc = np.zeros((cropped_height, cropped_width, 1))\n",
    "    \n",
    "    for x in range(0, cropped_width-1):\n",
    "        for y in range(0, cropped_height-1):\n",
    "            X_proc[y, x, 0] = X[y, x]\n",
    "    return X_proc\n",
    "\n",
    "# Generator to process data on demand and be able to process large amount of samples\n",
    "def generator(samples, batch_size=batch_size):\n",
    "    num_samples = len(samples)\n",
    "    counter = 0\n",
    "    while 1:\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            input_images = []\n",
    "            output_values = []\n",
    "            for batch_sample in batch_samples:\n",
    "                for i in range(3):\n",
    "                    image_file_name = batch_sample[i]\n",
    "                    image_full_filename = image_file_name.split('/')[-1]\n",
    "                    image_path = base_path + 'IMG/' + image_full_filename\n",
    "                    image = cv2.imread(image_path)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                    image = image[70:70 + cropped_height, 0:cropped_width]\n",
    "                    #plt.imshow(image, cmap = \"gray\")\n",
    "                    #plt.show()            \n",
    "                    input_images.append(image)\n",
    "\n",
    "                output_value_steering = float(batch_sample[3])\n",
    "                output_value_steering_left = output_value_steering + steering_correction\n",
    "                output_value_steering_right = output_value_steering - steering_correction\n",
    "                output_values.append(output_value_steering)\n",
    "                output_values.append(output_value_steering_left)\n",
    "                output_values.append(output_value_steering_right)\n",
    "\n",
    "            aug_input_images = []\n",
    "            aug_output_values = []\n",
    "            \n",
    "            for input_image, output_value in zip(input_images, output_values):\n",
    "                aug_input_images.append(input_image)\n",
    "                aug_output_values.append(output_value)\n",
    "                aug_input_images.append(cv2.flip(input_image, 1))\n",
    "                aug_output_values.append(-1.0 * output_value)\n",
    "                \n",
    "            # Free memory\n",
    "            input_images = []\n",
    "            output_values = []\n",
    "            \n",
    "            X_train_data = np.zeros((len(aug_input_images), cropped_height, cropped_width, 1))\n",
    "            for aug_input_image in aug_input_images:\n",
    "                X_train_data[counter] =  format_image(aug_input_image)\n",
    "\n",
    "            # convert to np arrays\n",
    "            y_train_data = np.array(aug_output_values)\n",
    "\n",
    "            # Free memory\n",
    "            aug_input_images = []\n",
    "            aug_output_values = []\n",
    "            \n",
    "            yield sklearn.utils.shuffle(X_train_data, y_train_data)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)\n",
    "\n",
    "reg_factor = 0.001\n",
    "\n",
    "#create a LeNet covnet with keras\n",
    "model = Sequential()\n",
    "# Normalize\n",
    "model.add(Lambda(lambda x: x/127.5 - 1.0, input_shape=(66,320,1)))\n",
    "\n",
    "# Add 3 * 5x5 convolution layers (output depth 24, 36, and 48), each with 2x2 stride\n",
    "model.add(Conv2D(24, (5, 5), strides=(2, 2), padding='valid', kernel_regularizer=l2(reg_factor), activation=\"relu\"))\n",
    "model.add(Conv2D(36, (5, 5), strides=(2, 2), padding='valid', kernel_regularizer=l2(reg_factor), activation=\"relu\"))\n",
    "model.add(Conv2D(48, (5, 5), strides=(2, 2), padding='valid', kernel_regularizer=l2(reg_factor), activation=\"relu\"))\n",
    "\n",
    "# Add 2 * 3x3 convolution layers (output depth 64, and 64)\n",
    "model.add(Conv2D(64, (3, 3), padding='valid', kernel_regularizer=l2(reg_factor), activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), padding='valid', kernel_regularizer=l2(reg_factor), activation=\"relu\"))\n",
    "\n",
    "# Add a flatten layer\n",
    "model.add(Flatten())\n",
    "# 4 Fully connected layers of 100, 50, 10 and 1\n",
    "model.add(Dense(100, kernel_regularizer=l2(reg_factor), activation=\"relu\"))\n",
    "model.add(Dense(50, kernel_regularizer=l2(reg_factor), activation=\"relu\"))\n",
    "model.add(Dense(10, kernel_regularizer=l2(reg_factor), activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Train covnet\n",
    "print('Training NN...')\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='mse')\n",
    "fit_history = model.fit_generator(train_generator, \n",
    "                                  steps_per_epoch=int(len(train_samples) / batch_size * factor_for_pre_process * 20), \n",
    "                                  validation_data=validation_generator,\n",
    "                                  validation_steps=int(len(validation_samples) / batch_size * factor_for_pre_process), epochs=6, verbose=1)\n",
    "\n",
    "# Plot cost history\n",
    "plt.plot(fit_history.history['loss'])\n",
    "plt.plot(fit_history.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Save model to be used later by drive.py\n",
    "model.save('model.h5')\n",
    "print(\"Using the 3 cameras, duplicated with mirroring, gray scale and 66 by 320 size images\")\n",
    "print('Trained model saved...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
